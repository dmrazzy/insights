import marimo

__generated_with = "0.15.3"
app = marimo.App(width="full")


@app.cell
def setup_pyoso():
    # This code sets up pyoso to be used as a database provider for this notebook
    # This code is autogenerated. Modification could lead to unexpected results :)
    import marimo as mo
    from pyoso import Client
    client = Client()
    try:
        pyoso_db_conn = client.dbapi_connection()    
    except Exception as e:
        pyoso_db_conn = None
    return client, mo


@app.cell
def about_app(mo):
    mo.vstack([
        mo.md("""
        # OSO Pipeline SLA Dashboard
        <small>Author: <span style="background-color: #f0f0f0; padding: 2px 4px; border-radius: 3px;">OSO Team</span> Â· Last Updated: <span style="background-color: #f0f0f0; padding: 2px 4px; border-radius: 3px;">2025-09-15</span></small>
        """),
        mo.md("""
        Monitor and track Service Level Agreement (SLA) compliance for OSO data pipeline models. This dashboard provides a heatmap visualization of model sync performance and calculates whether models are meeting their defined SLAs.
        """),
        mo.accordion({
            "<b>Click to see details on how app was made</b>": mo.accordion({
                "Methodology": """
                - **SLA Tracking**: Monitors sync frequency and timing for each model against defined SLA thresholds
                - **Heatmap Visualization**: Color-coded grid showing model performance across dates
                - **Compliance Calculation**: Determines if models are meeting their sync SLAs based on configurable criteria
                - **Performance Metrics**: Tracks sync success rates, delays, and overall pipeline health
                - **Alert System**: Identifies models that are consistently failing to meet SLA requirements
                """,
                "Data Sources": """
                - [Open Source Observer](https://opensource.observer/) - Pipeline sync data and model metadata
                - Internal pipeline monitoring systems - Real-time sync status and timing data
                - Model configuration tables - SLA definitions and thresholds
                """,
                "Further Resources": """
                - [Getting Started with Pyoso](https://docs.opensource.observer/docs/get-started/python)
                - [Using the Semantic Layer](https://docs.opensource.observer/docs/get-started/using-semantic-layer)
                - [Marimo Documentation](https://docs.marimo.io/)
                - [OSO Pipeline Documentation](https://docs.opensource.observer/docs/pipeline)
                """
            })
        })    
    ])
    return


@app.cell
def _(client):
    _df_staging_models = client.to_pandas("""
    WITH incremental_models AS (
      SELECT
        model_name,
        CASE      
          WHEN rendered_sql LIKE '%"time"%' THEN 'time'      
          WHEN rendered_sql LIKE '%"block_timestamp"%' THEN 'block_timestamp'
          WHEN rendered_sql LIKE '%"timestamp"%' THEN 'timestamp'
          WHEN rendered_sql LIKE '%"created_at"%' THEN 'created_at'
          WHEN rendered_sql LIKE '%"updated_at"%' THEN 'updated_at'
          WHEN rendered_sql LIKE '%"snapshot_at"%' THEN 'snapshot_at'
          WHEN rendered_sql LIKE '%"date"%' THEN 'date'
          WHEN rendered_sql LIKE '%"dt"%' THEN 'dt'
          ELSE NULL
        END AS time_column
      FROM models_v0
      WHERE model_name LIKE 'stg_%'
    )
    SELECT *
    FROM incremental_models
    WHERE time_column IS NOT NULL
    ORDER BY 1
    """)
    staging_models = _df_staging_models.set_index('model_name')['time_column'].to_dict()
    return (staging_models,)


@app.cell
def configuration_settings(datetime, mo, staging_models, timedelta):
    start_date_input = mo.ui.date(
        value=datetime.date.today() - timedelta(days=60),
        label="Start Date"
    )

    end_date_input = mo.ui.date(
        value=datetime.date.today(),
        label="End Date"
    )

    model_options = list(staging_models.keys()) if staging_models else []
    model_input = mo.ui.multiselect(
        options=model_options,
        value=[
            'stg_defillama__tvl_events',
            'stg_github__commits',
            #'stg_superchain__transactions',
            'stg_superchain__4337_userop_logs',
            'stg_growthepie__fundamentals_full',
        
        ],
        label="Select Models",
    )

    sla_threshold_input = mo.ui.number(
        start=1,
        stop=30,
        step=1,
        value=14,
        label="SLA Threshold (days)"
    )

    refresh_input = mo.ui.run_button(label="Analyze Models")

    mo.vstack([
        mo.md("### Configuration"),
        mo.hstack(
            [start_date_input, end_date_input, model_input, sla_threshold_input, refresh_input],
            justify="start",
            align="start",
            gap=1,
        )
    ])
    return (
        end_date_input,
        model_input,
        refresh_input,
        sla_threshold_input,
        start_date_input,
    )


@app.cell
def get_data(
    client,
    end_date_input,
    mo,
    model_input,
    pd,
    refresh_input,
    staging_models,
    start_date_input,
):
    mo.stop(not refresh_input.value)

    def query_model(model_name, time_column, start_date, end_date):
        query = f"""
        WITH date_series AS (
          SELECT date
          FROM UNNEST(
            SEQUENCE(DATE('{start_date}'), DATE('{end_date}'), INTERVAL '1' DAY)
          ) AS t(date)
        ),
        model_data AS (
          SELECT
            DATE(DATE_TRUNC('day', {time_column})) AS date,
            COUNT(*) AS row_count
          FROM {model_name}
          WHERE {time_column} BETWEEN DATE('{start_date}') AND DATE('{end_date}')
            GROUP BY 1
        )
        SELECT
          ds.date,
          COALESCE(md.row_count, 0) AS row_count
        FROM date_series ds
        LEFT JOIN model_data md ON ds.date = md.date
        ORDER BY ds.date DESC
        """
        df = client.to_pandas(query)
        df['model_name'] = model_name
        return df

    all_results = []

    selected_models = list(model_input.value)

    for model_name in mo.status.progress_bar(
        selected_models,
        title="Analyzing Models",
        subtitle="Querying model sync data...",
        show_eta=True,
        show_rate=True
    ):
        time_column = staging_models[model_name]
        try:
            model_data = query_model(
                model_name=model_name,
                time_column=time_column,
                start_date=start_date_input.value,
                end_date=end_date_input.value
            )
            all_results.append(model_data)
        except Exception as e:
            # Handle query errors gracefully
            print(f"Error querying {model_name}: {e}")
            continue

    if all_results:
        df_sync_data = pd.concat(all_results, ignore_index=True)
    else:
        df_sync_data = pd.DataFrame(columns=['model_name', 'date', 'row_count'])

    df_sync_data['row_count_norm'] = df_sync_data['row_count'] / df_sync_data.groupby('model_name')['row_count'].transform('max')
    return (df_sync_data,)


@app.cell
def process_sla_data(
    df_sync_data,
    end_date_input,
    np,
    pd,
    sla_threshold_input,
):
    _end_date = pd.to_datetime(end_date_input.value)
    _sla_threshold = int(sla_threshold_input.value)

    _df_sla = df_sync_data.copy()
    _df_sla['date'] = pd.to_datetime(_df_sla['date'])
    _df_sla['has_data'] = _df_sla['row_count'] > 0

    _df_asof = _df_sla[_df_sla['date'] <= _end_date]
    _last_ok = (
        _df_asof[_df_asof['has_data']]
        .groupby('model_name', as_index=False)['date']
        .max()
        .rename(columns={'date':'last_data_date'})
    )

    _max_rows = _df_sla.groupby('model_name')['row_count'].max().rename('max_rows')
    _med_rows = _df_sla[_df_sla['has_data']].groupby('model_name')['row_count'].median().rename('median_rows')
    _models = pd.concat([_max_rows, _med_rows], axis=1)

    df_sla = _models.merge(_last_ok, how='left', on='model_name')
    df_sla['days_since_data'] = (_end_date - df_sla['last_data_date']).dt.days
    df_sla['days_since_data'] = df_sla['days_since_data'].fillna(np.inf).astype('int64',errors='ignore')
    df_sla['meets_sla'] = df_sla['days_since_data'] <= _sla_threshold
    df_sla = df_sla.sort_values(['meets_sla','days_since_data','model_name'], ascending=[False,True,True])
    return (df_sla,)


@app.cell
def _(df_sla):
    df_summary = df_sla.groupby('model_name').agg({
        'meets_sla': ['count', 'sum', 'mean'],
        'days_since_data': 'min',
        'median_rows': 'sum'
    }).round(3)

    df_summary.columns = ['total_models', 'compliant_models', 'sla_compliance_rate', 'days_since_latest_data', 'median_rows']
    df_summary = df_summary.reset_index()
    return (df_summary,)


@app.cell
def generate_stats(df_summary, mo):
    total_models = mo.stat(
        label="Total Models",
        bordered=True,
        value=f"{df_summary['total_models'].sum():,.0f}",
    )

    compliant_models = mo.stat(
        label="Compliant Models",
        bordered=True,
        value=f"{df_summary['compliant_models'].sum():,.0f}",
    )

    avg_compliance = mo.stat(
        label="Average SLA Compliance",
        bordered=True,
        value=f"{df_summary['sla_compliance_rate'].mean():.1%}",
    )

    median_rows = mo.stat(
        label="Median Rows Processed",
        bordered=True,
        value=f"{df_summary['median_rows'].sum():,.0f}",
    )

    max_days_since_last_data = mo.stat(
        label="Max Days Since Last Run",
        bordered=True,
        value=f"{df_summary['days_since_latest_data'].max():,.0f}",
    )

    mo.hstack([total_models, compliant_models, avg_compliance, median_rows, max_days_since_last_data], widths="equal", gap=1)
    return


@app.cell
def generate_heatmap(df_sync_data, mo, px):
    def make_heatmap_fig(dataframe, title=""):
    
        df_numeric = dataframe.map(lambda x: round(x,3))

        fig = px.imshow(
            df_numeric,
            title=f"<b>{title}</b>",
            color_continuous_scale='Greens',
            aspect="auto"
        )

        fig.update_layout(
            paper_bgcolor="white",
            plot_bgcolor="white",
            font=dict(size=12, color="#111"),
            title=dict(text=title, x=0, xanchor="left"),
            margin=dict(t=0, l=20, r=20, b=20),
            xaxis_title="Date",
            yaxis_title="Model"
        )

        fig.update_xaxes(
            tickformat="%m/%d",
            ticks="outside",
            tickangle=-45
        )

        return fig

    _fig = make_heatmap_fig(
        df_sync_data.pivot_table(
            index='model_name',
            columns='date',
            values='row_count_norm',
            fill_value=0
        )
    )
    mo.vstack([
        mo.md("## Model Coverage Heatmap"),
        mo.md("*Coverage stats are normalized relative to the max row count for a given model*"),
        mo.ui.plotly(_fig)
    ])
    return


@app.cell
def generate_trend_plot(df_sync_data, mo, px):
    def make_trend_fig(daily_data, title=""):

        fig = px.line(
            daily_data,
            x='date',
            y='row_count_norm',
            color='model_name',
            title=f"<b>{title}</b>",
            markers=True,
            hover_data=['model_name', 'row_count']
        )

        fig.update_layout(
            paper_bgcolor="white",
            plot_bgcolor="white",
            font=dict(size=12, color="#111"),
            title=dict(text=title, x=0, xanchor="left"),
            margin=dict(t=0, l=20, r=20, b=20),
            yaxis_title="Rows Processed (Normalized)",
            xaxis_title="Date",
            hovermode='x unified',
        )

        fig.update_xaxes(
            showgrid=False,
            linecolor="#000",
            ticks="outside"
        )

        fig.update_yaxes(
            showgrid=True,
            gridcolor="#DDD",
            linecolor="#000",
            ticks="outside"
        )

        fig.update_traces(
            hovertemplate="Model: %{customdata[0]}<br>Rows: %{customdata[1]:,.0f}<extra></extra>"
        )

        return fig

    _fig = make_trend_fig(df_sync_data)
    mo.vstack([
        mo.md("## Data Processing Trend"),
        mo.ui.plotly(_fig)
    ])
    return


@app.cell
def generate_summary_table(df_summary, mo):
    df_display = df_summary.copy()
    df_display['meets_sla'] = df_display['sla_compliance_rate'].astype(bool)
    df_display['median_rows'] = df_display['median_rows'].apply(lambda x: f"{x:,.0f}")
    df_display = df_display[['model_name', 'meets_sla', 'median_rows', 'days_since_latest_data']]
    df_display.rename(columns={
        c: c.replace('_', ' ').title().replace('Sla', 'SLA')
        for c in df_display.columns
    }, inplace=True)

    mo.vstack([
        mo.md("### Model Performance Summary"),
        mo.ui.table(
            df_display.reset_index(drop=True),
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=25
        )
    ])
    return


@app.cell
def import_libraries():
    import datetime
    import pandas as pd
    import plotly.express as px
    import numpy as np
    from datetime import timedelta
    return datetime, np, pd, px, timedelta


if __name__ == "__main__":
    app.run()
